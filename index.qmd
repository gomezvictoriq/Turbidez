---
title: "Algoritmo para la estimación de la turbidez sobre el Río Paraná"


freeze: auto


format: 
  html:
    number-sections: true
    toc: true
    toc-location: left
    embed-resources: true
    crossrefs-hover: false
    lang: es
    bibliography: bibliografia/bibliografia.bib
    csl: bibliografia/ieee.csl
    theme: flatly
    page-layout: full
    css: styles/styles.css

    

date: last-modified
author:
  - name: Víctor Gustavo Gómez
    corresponding: true
    email: gomezvictoriq@ca.frre.utn.edu.ar
    affiliations:
      - name: GISTAQ (UTN-FRRe)
        url: https://www.instagram.com/gistaq.utn/
abstract: |
  Este sitio web contiene información sobre la estimación de la turbidez por teledetección en la cuenca media del Río Paraná. 
  La turbidez es uno parámetros de interés dentro proyecto Estimar indicadores de calidad de agua en la cuenca media del río Paraná para el desarrollo de un algoritmo mediante técnicas de teledetección satelital (MSECRE0008604), desarrollado por el Grupo de Investigación Sobre Temas Ambientales y Químicos (GISTAQ) de la Universidad Tecnológica Nacional Facultad Regional Resistencia (UTN-FRRe).

  Se utilizarán imágenes del satélite Sentinel-2 con corrección automática, de las cuales se obtiene la reflectancia de superficie del agua. Se buscará la relación entre la reflectancia y la turbidez por métodos de regresión tradionales y machine learning. Una vez obtenido el algoritmo que relacione ambas propiedades, se desarrollaran mapas de distribución espacial.
  
keywords:
  - GISTAQ
  - UTN
  - FRRe
  - Algoritmo
  - Turbidez
  - Machine learning
  - Teledetección
---

# Turbidez

La turbidez se refiere a la opacidad o falta de claridad en un líquido provocada por la presencia de partículas suspendidas. Este fenómeno es un indicador clave en el monitoreo de la calidad del agua y su influencia en diferentes ecosistemas es significativa.

La turbidez es un indicador de la calidad del agua, reflejando la presencia de partículas en suspensión. Su medición es crucial para garantizar la potabilidad del agua y la salud de los ecosistemas acuáticos. Este fenómeno puede ser resultado de diversas causas, como la erosión del suelo, la actividad biológica y la contaminación. La comprensión de la turbidez y su impacto es esencial para la gestión de recursos hídricos y la protección del medio ambiente.

La turbidez viene determinada por la dispersión de la luz causada por la materia suspendida en el agua, se obtiene normalmente mediante un turbidímetro, que proporciona medidas en Nephelometric Turbidity Unit (NTU) y mide la dispersión de un rayo de luz en el agua a 90º de la luz incidente [@Delegido2019].

Muchas propiedades, como la clorofila-a (Chl-a), sólidos suspendidos totales (SST) y la materia orgánica disuelta coloreada (CDOM), se utilizan a menudo como indicadores del estado del agua. Estos constituyentes del agua a su vez son responsables de la turbidez.

Existe una fuerte correlación entre turbidez y sólidos suspendidos totales, por lo que se puede estimar SST a partir de la turbidez. Por lo general, es una relación directa, a mayor concentración de SST mayor turbidez.

Existe una relación inversa entre la Turbidez y la profundidad del disco de Secchi (a valores bajos de secchi mayor turbidez), por lo que también se puede estimar turbidez a partir de mediciones de disco de secchi.

## Métodos tradicionales

:::: {.content-visible when-format="html"}

::: {.column-screen-right}
<!-- TODO corregir <br> de la ecuación -->
| Ecuación | Bandas (nm) | Métricas | Aguas | Plataforma | Referencia |
|:-:|:--|:--|:--|:--|:-:|
| $1.559e^{35.533 \cdot B03} \\ 1.879e^{37.745(B03 \cdot B5)/(B04+B12)}$ | B03, B04, B05, B12 | $R^{2}$, RMSE, MAE | Lago^[0,83 - 112,26 NTU.] | Sentinel-2 | @Ma2021 |
| $2677.2 \cdot B04^{1.856}$ | B04 | $R^{2}$, RMSE, Bias | Interiores variadas^[2,3 - 107,02 NTU.] | Landsat-8 | @Hossain2021 |
| $969-1.5468 \cdot R_{1200nm}+2.07 \frac{B8A}{B02}$ | B02, B8A, 1200nm | IOA, SI, RMSE, MAE | Río^[IOA = index of agreement<br>SI = scatter index.] | Landsat-8 | @Najafzadeh2023 |
| $y=-1.1+5.8 \frac{B02}{B04} \\ y=3.896-4.186 \frac{B02}{B03}$ | B02, B03, B04 | $R^{2}$, RMSE | Río^[20,6 - 112 NTU<br>2,3 - 15,4 NTU.] | Landsat-8 | @Allam2020 |
| $y=37661 \cdot B8A^{2}+1845 \cdot B8A <br> y=531.5- \frac{B04}{0.88}$ | B04, B8A | $R^{2}$, RMSE, MAPE | Estuario^[MAPE = Mean Absolute Percentage Error<br>0 - 1300 NTU<br>0 - 80 NTU.] | Pléiades | @Luo2020 |

: Características principales de algoritmos tradicionales para la estimación de turbidez. {#tbl-turb-trad .striped .hover tbl-colwidths="[40,15,15,10,10]"}

:::

::::


Múltiples modelos (lineal, logaritmos, inversa, cuadrática, exponencial, potencial) y plataformas (Sentinel-2, Landsat-5 y Landsat-8) emplean el cociente de bandas B04/B03 [@Shen2021].

Modelos de estimación a partir de Sentinel-2 y Landsat-8 utilizan regresiones lineales, cuadráticas y logarítmicas empleando B02, B03, B04, B01 (con menos apariciones) y cocientes entre éstas [@Ouma2020].

## Métodos de aprendizaje automático

El aprendizaje automático es un subconjunto de la inteligencia artificial que permite que un sistema aprenda y mejore de forma autónoma, sin necesidad de una programación explícita, a través del análisis de grandes cantidades de datos. El aprendizaje automático permite que los sistemas informáticos se ajusten y mejoren continuamente a medida que acumulan más "experiencias". Por lo tanto, el rendimiento de estos sistemas puede mejorar si se proporcionan conjuntos de datos más grandes y variados para su procesamiento.

Cuando se entrenan modelos de machine learning, cada conjunto de datos y cada modelo necesitan un conjunto diferente de "hiperparámetros".
Los hiperparámetros son variables de configuración externa que se utilizan para administrar el entrenamiento de modelos de machine learning. Controlan de forma directa la estructura, funciones y rendimiento de los modelos.
Los hiperparámetros son los parámetros de un modelo de aprendizaje automático, que no se aprenden durante el entrenamiento, sino que se establecen antes de que comience.

El "ajuste de hiperparámetros" permite modificar el rendimiento del modelo para lograr resultados óptimos. Este proceso es una parte fundamental del machine learning.
El ajuste de hiperparámetros puede ser manual o automático. A pesar de que el ajuste manual es lento y tedioso, permite entender mejor cómo afectan al modelo las ponderaciones de los hiperparámetros. El proceso de ajuste de hiperparámetros es iterativo, y debe probar diferentes combinaciones de parámetros y valores.

En el aprendizaje automático es importante utilizar técnicas de "validación cruzada" , de modo que el modelo no se centre únicamente en una única porción de sus datos.
La validación cruzada o cross-validation es una técnica utilizada para evaluar los resultados de un análisis estadístico y garantizar que son independientes de la partición entre datos de entrenamiento y prueba.
La idea básica de la validación cruzada es dividir los datos en conjuntos de entrenamiento y validación, y luego entrenar el modelo en el conjunto de entrenamiento y evaluar su rendimiento en el conjunto de validación. Este proceso se repite varias veces, con diferentes subconjuntos de los datos utilizados para el entrenamiento y la validación, y se calcula el rendimiento promedio.

En los procesos de machine learning supervisado se utilizan diversos algoritmos y técnicas de cálculo, generalmente calculados mediante el uso de programas como R o Python.

Dependiendo del tipo de datos que se usen para el entrenamiento, será de modelo de aprendizaje automático que se use.
A grandes rasgos, existen tres tipos de modelos que se usan en el aprendizaje automático: aprendizaje supervisado , no supervisado y por refuerzo.

Consultando el trabajo de otros investigadores, se observa que utilizan principalmente el aprendizaje automático supervisado.
Este tipo aprendizaje supervisado utiliza un conjunto de entrenamiento para enseñar a los modelos a producir el resultado deseado. Este conjunto de datos de entrenamiento incluye entradas y salidas correctas, que permiten al modelo aprender con el tiempo. El algoritmo mide su precisión a través de la función de pérdida, ajustando hasta que el error se haya minimizado lo suficiente.

Yang Zhe y otros, utilizaron como datos de entrada la reflectancia de superficie y datos de salida la turbidez, utilizaron los modelos SVR (support vector regression), random forest (RF) y eXtreme Gradiente Boostring (XGBoost).
Los hiperparámetros de cada modelo se determinaron mediante una búsqueda en cuadrícula de validación cruzada en Scikit-Learn de Python [@Yang2023].

Ma Yue y otros, utilizaron varios modelos de aprendizaje automático, usaron Python 3.7 tanto para la predicción de la turbidez del agua y la optimización de la los hiperparámetros [@Ma2021].

Zhao y otros probaron 14 modelos de machine learning en un estanque de peces con un dispositivo de construction propia, de los cuales ETR, Bagging, RFR, and ABR son los que presentaron un mejor desempeño en la estimación de la turbidez. Los algoritmos se implementaron utilizando Python 3.6 y bibliotecas de aprendizaje
scikit [@Zhao2022].

:::: {.content-visible when-format="html"}

::: {.column-screen-right}

|Modelo de machine learning|Cuerpo de agua|Métricas|Plataforma| Referencia |
|:--|:--|:--|:--|:-:|
|SVR, ELM ,BP ,CART ,GBT ,RF ,KNN|Lagos|RMSE, $R^{2}$, MAE|Sentinel-MSI|@Ma2021|
|eXtreme Gradient Boosting (XGBoost),  support vector regression (SVR), random forest (RF)|Lago|RMSE, $R^{2}$, MAPE| Sentinel-2A/B y Landsat-8/9 |  @Yang2023 |
| linear regression (LR), ridge regression (RR),  least absolute shrinkage and selection operator regression(LASSO), elastic net regression (ENR),  k-nearest neighbor regression (KNN), Gaussian process regression (GPR), decision tree regression (DTR), support vector regression (SVR), multilayer perceptron regression (MLP), adaptive boosting regression (ABR), gradient boosting regression (GBR), bootstrap aggregating regression (Bagging), random forest regression (RFR), and extreme tree regression (ETR) | Estanque de peces | MAE, MRSE, MAPE, $R^{2}$, RE, Acc |Propia| @Zhao2022 |

: Características principales de algoritmos de aprendizaje automático para la estimación de turbidez. {#tbl-turb-machine .striped .hover tbl-colwidths="[50,13,13,14,10]"}

:::

::::

```{python}

```

# Lectura y procesamiento de datos

Este sitio web está configurado de manera tal que se mantenga actualizado al tener disponibles nuevos datos de entrada, que serán procesados posteriormente. Mostrando siempre los resultados de todo el proceso. 

El código a continuación se dedica a la descarga automática de archivos desde el repositorio principal del proyecto.

```{python}
#| code-fold: true
from descarga_datos import main
main()
```

## Procesamiento de archivos csv

Para el procesamiento de los datos se utilizará la librería *pandas* de Python.

En el proyecto tenemos dos archivos .csv que contienen los datos:

-base_de_datos_lab.csv → contiene resultados de laboratorio 

-base_de_datos_gis.csv → contiene datos espectrales

```{python}
#| code-fold: true
import pandas as pd

#Lectura de datos

df1_lab = pd.read_csv(r"D:\GIT\Turbidez\Input\base_de_datos_lab.csv") # DataFrame de datos de laboratorio.
df2_gis = pd.read_csv(r"D:\GIT\Turbidez\Input\base_de_datos_gis_acolite.csv") # DataFrame de datos espectrales

# Combinamos ambos DataFrame para tener los datos en una única base de datos

df_combinado = pd.merge(df1_lab, df2_gis, on=['latitud', 'longitud','fecha'], how='inner')

# Filtramos las filas que contengas los valores de turbidez

df_turbidez = df_combinado[(df_combinado['param'] == 'turb')]

# Eliminamos las columnas que no nos interesan
df_turbidez_banda = df_turbidez.drop(columns=['longitud','latitud','punto','fecha','param'])

df_turbidez_banda.head()

# Cambiamos el nombre de la columna "valor" por el de "turbidez

df_turbidez_banda.rename(columns={'valor': 'turbidez'}, inplace=True)

df_turbidez_banda.head()

# Pivotamos tabla

df_final = df_turbidez_banda.pivot_table(
        index='turbidez',
        columns='banda',
        values='reflect',)

# Creamos tabla final

df_final.to_csv(r"D:\GIT\Turbidez\Output\csv\Turbidez-Bandas.csv", index=True)
```

La datos útiles a este sitio están ordenanos de la siguiente forma. (Solo se muestran las 5 primeras filas)

```{python}
#| echo: false
#| warning: false
#| message: false
import pandas as pd

df = pd.read_csv(r'D:\GIT\Turbidez\Output\csv\Turbidez-Bandas.csv')

df.head()
```

## Procesamiento de productos satelitales

Para el procesamiento se utilizará la librería *rasterio* en python.

Tabla de referencia para seleccionar bandas en rasterio

| Nombre de banda en archivo .tif | Banda de Sentinel-2 | Número para la lectura en rasterio |
|---------------------------------|---------------------|------------------------------------|
| Banda 1                         | B01                 | 1                                  |
| Banda 2                         | B02                 | 2                                  |
| Banda 3                         | B03                 | 3                                  |
| Banda 4                         | B04                 | 4                                  |
| Banda 5                         | B05                 | 5                                  |
| Banda 6                         | B06                 | 6                                  |
| Banda 7                         | B07                 | 7                                  |
| Banda 8                         | B08                 | 8                                  |
| Banda 9                         | B8A                 | 9                                  |
| Banda 10                        | B11                 | 10                                 |
| Banda 11                        | B12                 | 11                                 |

Lectura de imágen raster y visualización del área de estudio, con el objetivo de verificar que el código está funcionando y de que se leen correctamente los archivos.

```{python}
#| echo: false
#| warning: false
#| message: false
import rasterio as rs
import matplotlib.pyplot as plt
import numpy as np

ruta = r"D:\GIT\Turbidez\Input\recorte_acolite\2025-06-09.tif"

# Abrir el archivo
with rs.open(ruta) as raster:

    #Definimos las bandas 
    #Bandas RGB para ver área de estudio
    B02 = raster.read(2) #Azul
    B03 = raster.read(3) #Verde
    B04 = raster.read(4) #Rojo

    #Para calcular NDWI con B03 y B11
    B11 = raster.read(10)

    #para el modelo
    B05 = raster.read(5)
    
# Composión de imagen 
rgb = np.stack([B04, B03, B02], axis=-1)

# Mejorar constraste
def stretch(banda):
    p2, p98 = np.percentile(banda, (2, 98))
    return np.clip((banda - p2) / (p98 - p2 + 1e-6), 0, 1)

rgb_stretched = np.stack([stretch(B04), stretch(B03), stretch(B02)], axis=-1)

# Gráfico
plt.figure(figsize=(4.5, 4.5))
plt.imshow(rgb_stretched)
plt.title("Río Paraná")
plt.axis("off")
plt.tight_layout()
plt.show()
```

Se utilizrá el índice NDWI para recortar cada la imágen satelital, y poder trabajar únicamente con los píxeles de agua.

$NDWI=(B03-B11)/(B03+B11)$

```{python, echo=FALSE}
#| code-fold: true
import numpy as np
import matplotlib.pyplot as plt
from skimage.filters import threshold_otsu

# NDWI
ndwi = (B03 - B11) / (B03 + B11 + 1e-6)

# Calcular umbral automático con Otsu
umbral = threshold_otsu(ndwi[np.isfinite(ndwi)])  # descartamos NaN e inf

# Binarización: agua = 1, no agua = 0
ndwi_bin = np.where(ndwi >= umbral, 1, 0)

# Gráfico máscara de agua
fig, ax = plt.subplots(figsize=(4.5, 4.5))
img = ax.imshow(ndwi_bin, cmap='Greys')
ax.set_title(f"NDWI - Otsu (umbral={umbral:.3f})")
cbar = fig.colorbar(img, ax=ax, fraction=0.046, pad=0.04)
cbar.set_label("Agua (1) / No Agua (0)")
ax.tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)
ax.axis("on")
plt.tight_layout()
plt.show()
```

Durante la verificacion de los recortes de las imágenes satélitales, se observó que con el método Otsu no se puede calcular correctamente el valor umbral **agua/no agua** , por ende no se puede procesar correctamente algunas imágenes. Para resolver esto se decidió utilizar el valor promedio del NDWI para calcular el valor umbral, el cual sí puede hacer una mejor demarcación de las zona agua/no agua.

Una vez entrenado y validado los modelos, se hará automáticamente la lectura de archivos **.tif** de entrada, a cada una se realizara el cálculo de umbral de máscara de agua y su procesamiento para generar mapas de turbidez, 

## Pruebas de correlación

Para ser más rigurosos, agregaremos ésta etapaa durante el entrenamiento de nuestro modelo lineal.

En esta primera etapa nos interesa estimar la turbidez por medio de modelos lineales sencillos, para encontrar mejores modelos podemos aplicar transformaciones logarítmicas con el objetivo principal de linealizar relaciones no lineales.

Primeramente se probará un modelo sin aplicar ninguna transformaciones en las variables, luego se aplicará logaritmo natural en la turbidez, en las bandas espectrales y en en ambas varaibles, con el fin de evaluar en rendimiento de los modelos y seleccionar los mejores, según sus métricas de desempeño.

Se utilizará el coeficiente de correlación lineal **r**, para evaluar la relación lineal entre la turdidez y las distintas bandas (con y sin transformación logarítmica).

Calculamos coeficiente de correlacion lineal **r** entre la turbidez y cada banda con la función *.corr* de pandas.

Esta medida indica cuanto se relacionan dos variables, puede tomar valores desde -1 a +1: 

• +1 correlación lineal perfecta positiva.

•  0 sin correlación.

• -1 correlación lineal perfecta negativa.


**Prueba de correlación C1: (sin aplicar logaritmo)**
 
Cálculo de correlación lineal entre la turbidez y las bandas espectrales.

```{python}
#| code-fold: true

import pandas as pd

Datos= pd.read_csv(r'D:\GIT\Turbidez\Output\csv\Turbidez-Bandas.csv')

bandas = [col for col in Datos.columns if col.startswith('B')]

correlaciones = {}

for banda in bandas:
    r = Datos['turbidez'].corr(Datos[banda])
    correlaciones[banda] = r

#Creamos un Data Frame.
df_correlaciones1 = pd.DataFrame(list(correlaciones.items()), columns=['Banda', 'r'])
#Ordenamos por mayor a menos valor de r.
df_correlaciones1 = df_correlaciones1.sort_values(by='r', ascending=False).reset_index(drop=True)

df_correlaciones1['Banda'] = "turb vs " + df_correlaciones1['Banda'].astype(str)
df_correlaciones1.rename(columns={'Banda': 'Combinación 1'}, inplace=True)

df_correlaciones1.to_csv(r'D:\GIT\Turbidez\Output\correlaciones\C1_turb_vs_banda.csv', index=False)

#df_correlaciones1
```

**Prueba de correlación C2: logaritmo en la turbidez**

El código a continuación aplica logaritmo a la columna de turbidez.

```{python}
#| code-fold: true
import numpy as np

Datos_turb_log = pd.read_csv(r'D:\GIT\Turbidez\Output\csv\Turbidez-Bandas.csv')
Datos_turb_log['turbidez'] = np.log(Datos_turb_log['turbidez'])

#Cambio el nombre la columna "turbidez" luego de aplicar el logaritmo
Datos_turb_log = Datos_turb_log.rename(columns={'turbidez': 'ln_turbidez'})

#Creo archivo csv para usarlo para entrenar modelos
Datos_turb_log.to_csv(r"D:\GIT\Turbidez\Output\csv\ln_turb-bandas.csv", index=False)

```

Cálculo de correlación lineal  entre el logaritmo de la turbidez y las bandas espectrales.

```{python}
#| code-fold: true
bandas = [col for col in Datos_turb_log.columns if col.startswith('B')]

correlaciones = {}

for banda in bandas:
    r = Datos_turb_log['ln_turbidez'].corr(Datos_turb_log[banda])
    correlaciones[banda] = r

#Creamos un Data Frame.
df_correlaciones2 = pd.DataFrame(list(correlaciones.items()), columns=['Banda', 'r'])
#Ordenamos por mayor a menos valor de r.
df_correlaciones2 = df_correlaciones2.sort_values(by='r', ascending=False).reset_index(drop=True)

df_correlaciones2['Banda'] = "ln_turb vs " + df_correlaciones2['Banda'].astype(str)
df_correlaciones2.rename(columns={'Banda': 'Combinación 2'}, inplace=True)

df_correlaciones2.to_csv(r'D:\GIT\Turbidez\Output\correlaciones\C2_ln_turb_vs_banda.csv', index=False)

```

**Prueba de correlación C3: logaritmo en las bandas**

Anteriormente en el proyecto se estaba trabajando con una base de datos de reflectancias procesadas con Sen2Cor. Actualmente trabajamos con una nueva base de datos con las reflectancias corregidas por Acolite.

Acolite y Sen2Cor son dos herramientas de software utilizadas para la corrección atmosférica de imágenes satelitales. Acolite se destaca en ambientes acuáticos eutróficos, mientras que Sen2Cor es un procesador más general para la creación de productos Sentinel-2 de nivel 2A.

**IMPORTANTE:** Este cambio provocó errores en el código a partir de esta sección en adelante, el error se debe a que hay valores de reflectancia que son cero y negativos en la banda 12, en los datos provenientes de ACOLITE.

En las pruebas de correlación C3 y C4 aplicamos logaritmo a las bandas. Por definción de logaritmo no ponemos tomar valores cero ni negativos. Para salvar este error, se procedió a filtrar filas con esos valores.

Creamos un nuevo DataFrame para aplicarle el logaritmo a las bandas. (Se usará tambien para la prueba de correlación 4)

```{python}
#| code-fold: true

import pandas as pd

# Leer los datos
Datos = pd.read_csv(r'D:\GIT\Turbidez\Output\csv\Turbidez-Bandas.csv')

# Filtrar filas: nos quedamos solo con aquellas donde **todos los valores sean mayores a 0**
Datos_filtrados = Datos[(Datos > 0).all(axis=1)]


Datos_filtrados.to_csv(r"D:\GIT\Turbidez\Output\csv\Datos_filtrados-para-usar-ln.csv", index=False)

```

Aplicamos logaritmo a las bandas

```{python}
#| code-fold: true

# Este chunk sólo se usa para aplicar logaritmo a datos_filtrados 
import numpy as np

DatosC3 = Datos_filtrados.copy()

col = [col for col in DatosC3.columns if col.startswith('B')]

DatosC3[col] = np.log(DatosC3[col])

#Cambiamos en nombre las columnas, agremamos ln_ a cada columna

DatosC3.columns = ['ln_' + col for col in DatosC3.columns]

DatosC3.rename(columns={'ln_turbidez': 'turbidez'}, inplace=True)

#Creo archivo csv para usarlo para entrenar modelos
DatosC3.to_csv(r"D:\GIT\Turbidez\Output\correlaciones\C3_turb_vs_ln_banda.csv", index=False)

```

Cálculo de correlación lineal entre la turbidez y el logaritmo de las bandas espectrales. 

```{python}
#| code-fold: true
bandas_ln = [col for col in DatosC3.columns if col.startswith('ln_B')]

correlaciones = {}

for banda in bandas_ln:
    r = DatosC3['turbidez'].corr(DatosC3[banda])
    correlaciones[banda] = r

#Creamos un Data Frame.
df_correlaciones3 = pd.DataFrame(list(correlaciones.items()), columns=['Banda', 'r'])
#Ordenamos por mayor a menos valor de r.
df_correlaciones3 = df_correlaciones3.sort_values(by='r', ascending=False).reset_index(drop=True)

df_correlaciones3['Banda'] = "turb vs " + df_correlaciones3['Banda'].astype(str)
df_correlaciones3.rename(columns={'Banda': 'Combinación 3'}, inplace=True)


df_correlaciones3.to_csv(r'D:\GIT\Turbidez\Output\correlaciones\C3_turb_vs_ln_banda.csv', index=False)

```

**Prueba de correlación C4: logaritmo en turbidez y en bandas**

```{python}
#| code-fold: true
#| 
import numpy as np

DatosC4 = np.log(Datos_filtrados)
#Cambiamos en nombre las columnas, agremamos ln_ a cada columna
DatosC4.columns = ['ln_' + col for col in DatosC4.columns]

#Creo archivo csv para usarlo para entrenar modelos
DatosC4.to_csv(r"D:\GIT\Turbidez\Output\csv\ln_turb-ln_banda.csv", index=False)

```

Cálculo de correlación lineal entre el logaritmo de la  turbidez y el logaritmo de las bandas espectrales.

```{python}
#| code-fold: true

bandas_ln = [col for col in DatosC4.columns if col.startswith('ln_B')]

correlaciones = {}

for banda in bandas_ln:
    r = DatosC4['ln_turbidez'].corr(DatosC4[banda])
    correlaciones[banda] = r

#Creamos un Data Frame.
df_correlaciones4 = pd.DataFrame(list(correlaciones.items()), columns=['Banda', 'r'])
#Ordenamos por mayor a menos valor de r.
df_correlaciones4 = df_correlaciones4.sort_values(by='r', ascending=False).reset_index(drop=True)

df_correlaciones4['Banda'] = "ln_turb vs " + df_correlaciones4['Banda'].astype(str)
df_correlaciones4.rename(columns={'Banda': 'Combinación 4'}, inplace=True)

df_correlaciones4.to_csv(r'D:\GIT\Turbidez\Output\correlaciones\C4_ln_turb_vs_ln_banda.csv', index=False)

```

Se resume en una tabla las pruebas de correlación realizadas

**Tabla resúmen de pruebas de correlación lineal **r** **

```{python}
#| echo: false
#| warning: false
#| message: false
import pandas as pd 

# Aumentar el ancho total permitido
pd.set_option('display.width', 2000)
pd.set_option('display.max_columns', None)

C1 = pd.read_csv(r"D:\GIT\Turbidez\Output\correlaciones\C1_turb_vs_banda.csv")
C2= pd.read_csv(r"D:\GIT\Turbidez\Output\correlaciones\C2_ln_turb_vs_banda.csv")
C3 = pd.read_csv(r"D:\GIT\Turbidez\Output\correlaciones\C3_turb_vs_ln_banda.csv")
C4 = pd.read_csv(r"D:\GIT\Turbidez\Output\correlaciones\C4_ln_turb_vs_ln_banda.csv")

C_comb = pd.concat([C1, C2 ,C3, C4], axis=1)

C_comb
```

## Seleción de variables

A partir del análisis con las correlaciones entre turbidez (y su logaritmo) y distintas bandas (y sus logaritmos). Se puede observar que en las combinaciones 1 y 4 las correlaciones son mas altas respecto a C2 y C3. Por que que se seguirá el análisis con estas combinaciones:

  • Combinación 1: turb vs bandas

  • Combinación 4: ln_turb vs ln_bandas

Se entrenarán modelos con ambas combinaciones por separado y se evalurá su desempeño.

Todos los modelos serán sometidos a bootstrapping con enfoque out-of-bag (OOB):

→ Donde se tomará el 75% de los datos ***con reemplazo*** para entrenamiento;
→ Luego se usa el 25% restante (no muestreados) como conjunto de prueba (test).

Proponemos avanzar hacia un modelo multivariable, utilizando el *AIC* @Chatterjee2015 como criterio de seleccion de variables, este criterio es muy conveniente para encontrar el modelo de mejor balance entre ajuste y complejidad.

**¿Qué hace el AIC?**

→ Penaliza modelos con demasiadas variables para evitar sobreajuste.

→ Se busca minimizar el AIC: cuanto más bajo, mejor el modelo.

→ No requiere que las variables tengan alta correlación individual, pero sí que mejoren el modelo conjunto.

AIC=n⋅ln(RMSE²)+2k

### Combinación 1: turb vs bandas

```{python}
#| echo: false
#| warning: false
#| message: false

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from bootstrap import run_bootstrap_linear_regression_analysis 

# Leer los datos
Datos = pd.read_csv(r"D:\GIT\Turbidez\Output\csv\Turbidez-Bandas.csv")
y = Datos['turbidez']

# Variables
variables_fijas = ['B05']
variables_a_agregar = ['B06', 'B08', 'B07', 'B04', 'B8A', 'B03', 'B02', 'B01', 'B12', 'B11']

# Resultados
resultados = []

# Configuración de bootstrapping
n_iteraciones_bootstrap = 1000

# Entrenamiento incremental
for i in range(len(variables_a_agregar) + 1):
    variables_usadas = variables_fijas + variables_a_agregar[:i]
    X = Datos[variables_usadas].values

    # División entrenamiento/test
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.25, shuffle=True, random_state=42
    )

    # Bootstrapping: obtener coeficientes promedio y métricas de entrenamiento
    coef_prom, intercept_prom, r2_train_boot, rmse_train_boot = run_bootstrap_linear_regression_analysis(
    X_train, y_train.to_numpy(), n_iteraciones=n_iteraciones_bootstrap)


    # Modelo final con coeficientes promedio
    modelo_final = LinearRegression()
    modelo_final.coef_ = coef_prom
    modelo_final.intercept_ = intercept_prom

    # Predicción sobre testeo
    y_pred = modelo_final.predict(X_test)
    r2 = r2_score(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))

    # R² ajustado
    n_obs = len(y_test)
    n_vars = X_test.shape[1]
    r2_ajustado = 1 - (1 - r2) * (n_obs - 1) / (n_obs - n_vars - 1)

    # AIC
    residuals = y_test - y_pred
    rss = np.sum(residuals ** 2)
    k = X_test.shape[1] + 1  # +1 por el intercepto
    aic = n_obs * np.log(rss / n_obs) + 2 * k

    # Guardar resultados
    resultados.append({
        "variables": ", ".join(variables_usadas),
        "num_variables": len(variables_usadas),
        "R²_train (bootstrap)": r2_train_boot,
        "RMSE_train (bootstrap)": rmse_train_boot,
        "R²_test": r2,
        "R²-ajustado": r2_ajustado,
        "RMSE_test": rmse,
        "AIC": aic
    })

# Convertir a DataFrame
df_resultados = pd.DataFrame(resultados)
df_resultados

```

**Gráfico de AIC**

```{python}
#| echo: false
#| warning: false
#| message: false
aic_scores = [r["AIC"] for r in resultados]
n_vars = [r["num_variables"] for r in resultados]

plt.plot(n_vars, aic_scores, marker='o', color='purple')
plt.title('AIC vs Número de Variables')
plt.xlabel('Número de variables')
plt.ylabel('AIC')
plt.grid(True)
plt.show()
```

¿Por qué el AIC baja al agregar más variables, si se supone que penaliza la complejidad?

AIC=n⋅ln(RMSE²)+2k

donde:

n = número de observaciones
k = número de parámetros (incluye el intercepto)
ln(RMSE²) representa el ajuste del modelo

AIC no busca modelos simples en sí, sino la mejor compensación entre ajuste y complejidad.

A pesar de agregar más variables, el error (RMSE) está disminuyendo mucho más rápido de lo que penaliza el AIC con +2k


### Combinación 4: ln_turb vs ln_bandas

```{python}
#| echo: false
#| warning: false
#| message: false

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from bootstrap import run_bootstrap_linear_regression_analysis  # Asegurate de que esta función devuelve lo necesario

# Leer los datos
Datos = pd.read_csv(r"D:\GIT\Turbidez\Output\csv\ln_turb-ln_banda.csv")
y = Datos['ln_turbidez']

# Variables
variables_fijas = ['ln_B05']
variables_a_agregar = ['ln_B04', 'ln_B06', 'ln_B07', 'ln_B08', 'ln_B8A', 'ln_B03', 'ln_B02', 'ln_B01','ln_B11' ,'ln_B12']

# Resultados
resultados = []

# Configuración de bootstrapping
n_iteraciones_bootstrap = 1000

# Entrenamiento incremental
for i in range(len(variables_a_agregar) + 1):
    variables_usadas = variables_fijas + variables_a_agregar[:i]
    X = Datos[variables_usadas].values

    # División entrenamiento/test
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.25, shuffle=True, random_state=42
    )

    # Bootstrapping: obtener coeficientes promedio y métricas de entrenamiento
    coef_prom, intercept_prom, r2_train_boot, rmse_train_boot = run_bootstrap_linear_regression_analysis(
    X_train, y_train.to_numpy(), n_iteraciones=n_iteraciones_bootstrap)


    # Modelo final con coeficientes promedio
    modelo_final = LinearRegression()
    modelo_final.coef_ = coef_prom
    modelo_final.intercept_ = intercept_prom

    # Predicción sobre testeo
    y_pred = modelo_final.predict(X_test)
    r2 = r2_score(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))

    # Cálculo R² ajustado
    n_obs = len(y_test)
    n_vars = X_test.shape[1]

    if n_obs - n_vars - 1 != 0:
     r2_ajustado = 1 - (1 - r2) * (n_obs - 1) / (n_obs - n_vars - 1)
    else:
     r2_ajustado = np.nan  # o podrías usar 0.0 si preferís



    # AIC
    residuals = y_test - y_pred
    rss = np.sum(residuals ** 2)
    k = X_test.shape[1] + 1  # +1 por el intercepto
    aic = n_obs * np.log(rss / n_obs) + 2 * k

    # Guardar resultados
    resultados.append({
        "variables": ", ".join(variables_usadas),
        "num_variables": len(variables_usadas),
        "R²_train (bootstrap)": r2_train_boot,
        "RMSE_train (bootstrap)": rmse_train_boot,
        "R²_test": r2,
        "R²-ajustado": r2_ajustado,
        "RMSE_test": rmse,
        "AIC": aic
    })

# Convertir a DataFrame
df_resultados = pd.DataFrame(resultados)
df_resultados
```

**Gráfico de AIC**
```{python}
#| echo: false
#| warning: false
#| message: false
aic_scores = [r["AIC"] for r in resultados]
n_vars = [r["num_variables"] for r in resultados]

plt.plot(n_vars, aic_scores, marker='o', color='purple')
plt.title('AIC vs Número de Variables')
plt.xlabel('Número de variables')
plt.ylabel('AIC')
plt.grid(True)
plt.show()
```

# Entrenamiento de modelo lineal 

A continuación se presentan los modelos lineales candidatos para estimar la turbidez, cada uno con sus respectivas métricas de desempeño.

## Modelos lineales

### Modelo lineal 1 

Utiliza B05, para estimar tubidez

```{python}
#| echo: false
#| warning: false
#| message: false
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from bootstrap import run_bootstrap_linear_regression_analysis

# Leer datos
df = pd.read_csv(r"D:\GIT\Turbidez\Output\csv\Turbidez-Bandas.csv")

# Variables predictoras
variables = ['B05']
X_completo = df[variables].values
y_completo = df['turbidez'].values

# Separar entrenamiento y testeo
X_train, X_test, y_train, y_test = train_test_split(
    X_completo, y_completo, test_size=0.25, random_state=42
)

# Ejecutar bootstrapping
n_iteraciones_config = 1000
coef_prom, intercept_prom, r2_train_boot, rmse_train_boot = \
    run_bootstrap_linear_regression_analysis(X_train, y_train, n_iteraciones=n_iteraciones_config)

# ------------------ Guardar coeficientes del modelo B05 ------------------ #
coef_B05 = coef_prom[0]         # coeficiente de la banda B05
intercept_B05 = intercept_prom  # intercepto

# Ecuación en LaTeX (solo para mostrar)
ecuacion_latexB05 = f"turbidez = {coef_B05:.4f} \\cdot B05 + {intercept_B05:.4f}"

from IPython.display import display, Math
display(Math(ecuacion_latexB05))


# Modelo final con coeficientes promedio
modelo_final_promedio = LinearRegression()
modelo_final_promedio.coef_ = coef_prom
modelo_final_promedio.intercept_ = intercept_prom

# Predicción sobre entrenamiento y test
y_train_pred = modelo_final_promedio.predict(X_train)
y_test_pred = modelo_final_promedio.predict(X_test)

# Métricas en testeo
r2_test = r2_score(y_test, y_test_pred)
rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))

# ------------------ Gráfico con estilo personalizado ------------------ #
plt.figure(figsize=(4.5, 4.5))

# Entrenamiento
plt.scatter(y_train, y_train_pred, color="#9D50A6", alpha=0.5, label="Datos de entrenamiento", marker='o')

# Test
plt.scatter(y_test, y_test_pred, color="red", alpha=0.7, label="Datos de testeo", marker='^')

# Línea ideal
min_val = min(np.min(y_train), np.min(y_test))
max_val = max(np.max(y_train), np.max(y_test))
plt.plot([min_val, max_val], [min_val, max_val], '--', color="#17A77E", lw=2, label="Línea ideal")

# Línea de tendencia sobre test
coef_linea = np.polyfit(y_test, y_test_pred, 1)  # [pendiente, intercepto]
x_tendencia = np.linspace(min_val, max_val, 100)
y_tendencia = coef_linea[0] * x_tendencia + coef_linea[1]
plt.plot(x_tendencia, y_tendencia, '-', color='black', lw=2, label="Línea de tendencia")

plt.xlabel("Turbidez real (NTU)")
plt.ylabel("Turbidez estimada (NTU)")
plt.title(
    f"Regresión lineal con Bootstrapping ({n_iteraciones_config} modelos)\n"
    f"R² entrenamiento: {r2_train_boot:.4f}, RMSE: {rmse_train_boot:.4f} | "
    f"R² testeo: {r2_test:.4f}, RMSE: {rmse_test:.4f}"
)
plt.legend()
plt.grid(True, linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

# ------------------ Coeficientes ------------------ #

# Diccionario para renombrar las variables en notación matemática
nombre_vars_latex = {
    'B05': 'B05'
}

# Construir términos con nombres renombrados
terminos_latex = [
    f"{coef:.4f} \\cdot {nombre_vars_latex[var]}"
    for var, coef in zip(variables, coef_prom)
]

# Construir la ecuación completa en LaTeX
ecuacion_latexB05 = " + ".join(terminos_latex)
ecuacion_latexB05 = f"turbidez = {ecuacion_latexB05} + {intercept_prom:.4f}"

display(Math(ecuacion_latexB05))

```

### Modelo lineal 2

Utiliza 6 bandas para estimar la turbidez: B04, B05, B06, B07, B08 y B8A

```{python}
#| echo: false
#| warning: false
#| message: false
#| 
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from bootstrap import run_bootstrap_linear_regression_analysis

# Leer datos
df = pd.read_csv(r"D:\GIT\Turbidez\Output\csv\Turbidez-Bandas.csv")

# Variables predictoras
variables = ['B05', 'B06', 'B08', 'B07', 'B04', 'B8A']
X_completo = df[variables].values
y_completo = df['turbidez'].values

# Separar entrenamiento y testeo
X_train, X_test, y_train, y_test = train_test_split(
    X_completo, y_completo, test_size=0.25, random_state=42
)

# Ejecutar bootstrapping
n_iteraciones_config = 1000
coef_prom, intercept_prom, r2_train_boot, rmse_train_boot = \
    run_bootstrap_linear_regression_analysis(X_train, y_train, n_iteraciones=n_iteraciones_config)

# Modelo final con coeficientes promedio
modelo_final_promedio = LinearRegression()
modelo_final_promedio.coef_ = coef_prom
modelo_final_promedio.intercept_ = intercept_prom

# Predicción sobre entrenamiento y test
y_train_pred = modelo_final_promedio.predict(X_train)
y_test_pred = modelo_final_promedio.predict(X_test)

# Métricas en testeo
r2_test = r2_score(y_test, y_test_pred)
rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))

# ------------------ Gráfico con estilo personalizado ------------------ #
plt.figure(figsize=(4.5, 4.5))

# Entrenamiento
plt.scatter(y_train, y_train_pred, color="#9D50A6", alpha=0.5, label="Datos de entrenamiento", marker='o')

# Test
plt.scatter(y_test, y_test_pred, color="red", alpha=0.7, label="Datos de testeo", marker='^')

# Línea ideal
min_val = min(np.min(y_train), np.min(y_test))
max_val = max(np.max(y_train), np.max(y_test))
plt.plot([min_val, max_val], [min_val, max_val], '--', color="#17A77E", lw=2, label="Línea ideal")

# Línea de tendencia sobre test
coef_linea = np.polyfit(y_test, y_test_pred, 1)  # [pendiente, intercepto]
x_tendencia = np.linspace(min_val, max_val, 100)
y_tendencia = coef_linea[0] * x_tendencia + coef_linea[1]
plt.plot(x_tendencia, y_tendencia, '-', color='black', lw=2, label="Línea de tendencia")

plt.xlabel("Turbidez real (NTU)")
plt.ylabel("Turbidez estimada (NTU)")
plt.title(
    f"Regresión lineal con Bootstrapping ({n_iteraciones_config} modelos)\n"
    f"R² entrenamiento: {r2_train_boot:.4f}, RMSE: {rmse_train_boot:.4f} | "
    f"R² testeo: {r2_test:.4f}, RMSE: {rmse_test:.4f}"
)
plt.legend()
plt.grid(True, linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

# ------------------ Coeficientes ------------------ #

# Diccionario para renombrar las variables en notación matemática
nombre_vars_latex = {
    'B05': 'B05',
    'B04': 'B04',
    'B06': 'B06',
    'B07': 'B07',
    'B8A': 'B8A',
    'B08': 'B08'
}

# Construir términos con nombres renombrados
terminos_latex = [
    f"{coef:.4f} \\cdot {nombre_vars_latex[var]}"
    for var, coef in zip(variables, coef_prom)
]

# Construir la ecuación completa en LaTeX
ecuacion_latex = " + ".join(terminos_latex)
ecuacion_latex = f"turbidez = {ecuacion_latex} + {intercept_prom:.4f}"

from IPython.display import display, Math
display(Math(ecuacion_latex))
```

### Modelo lineal 3

Utiliza el logaritmo de la turbidez y en la banda B05

**Escala logarítmica**
```{python}
#| echo: false
#| warning: false
#| message: false
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from bootstrap import run_bootstrap_linear_regression_analysis

# Leer datos
df = pd.read_csv(r"D:\GIT\Turbidez\Output\csv\ln_turb-ln_banda.csv")

# Variables predictoras
variables = ['ln_B05']
X_completo = df[variables].values
y_completo = df['ln_turbidez'].values

# Separar entrenamiento y testeo
X_train, X_test, y_train, y_test = train_test_split(
    X_completo, y_completo, test_size=0.25, random_state=42
)

# Ejecutar bootstrapping
n_iteraciones_config = 1000
coef_prom, intercept_prom, r2_train_boot, rmse_train_boot = \
    run_bootstrap_linear_regression_analysis(X_train, y_train, n_iteraciones=n_iteraciones_config)

# Modelo final con coeficientes promedio
modelo_final_promedio = LinearRegression()
modelo_final_promedio.coef_ = coef_prom
modelo_final_promedio.intercept_ = intercept_prom

# Predicción sobre entrenamiento y test
y_train_pred = modelo_final_promedio.predict(X_train)
y_test_pred = modelo_final_promedio.predict(X_test)

# Métricas en testeo
r2_test = r2_score(y_test, y_test_pred)
rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))

# ------------------ Gráfico con estilo personalizado ------------------ #
plt.figure(figsize=(4.5, 4.5))

# Entrenamiento
plt.scatter(y_train, y_train_pred, color="#9D50A6", alpha=0.5, label="Datos de entrenamiento", marker='o')

# Test
plt.scatter(y_test, y_test_pred, color="red", alpha=0.7, label="Datos de testeo", marker='^')

# Línea ideal
min_val = min(np.min(y_train), np.min(y_test))
max_val = max(np.max(y_train), np.max(y_test))
plt.plot([min_val, max_val], [min_val, max_val], '--', color="#17A77E", lw=2, label="Línea ideal")

# Línea de tendencia sobre test
coef_linea = np.polyfit(y_test, y_test_pred, 1)  # [pendiente, intercepto]
x_tendencia = np.linspace(min_val, max_val, 100)
y_tendencia = coef_linea[0] * x_tendencia + coef_linea[1]
plt.plot(x_tendencia, y_tendencia, '-', color='black', lw=2, label="Línea de tendencia")

plt.xlabel("ln(Turbidez) medida (NTU)")
plt.ylabel("ln(Turbidez) estimada (NTU)")
plt.title(
    f"Regresión lineal con Bootstrapping ({n_iteraciones_config} modelos)\n"
    f"R² entrenamiento: {r2_train_boot:.4f}, RMSE: {rmse_train_boot:.4f} | "
    f"R² testeo: {r2_test:.4f}, RMSE: {rmse_test:.4f}"
)
plt.legend()
plt.grid(True, linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

# ------------------ Coeficientes ------------------ #

# Diccionario para renombrar las variables en notación matemática
nombre_vars_latex = {
    'ln_B05': 'ln(B05)',
    'ln_B04': 'ln(B04)',
    'ln_B06': 'ln(B06)'
}

# Construir términos con nombres renombrados
terminos_latex = [
    f"{coef:.4f} \\cdot {nombre_vars_latex[var]}"
    for var, coef in zip(variables, coef_prom)
]

# Construir la ecuación completa en LaTeX
ecuacion_latex = " + ".join(terminos_latex)
ecuacion_latex = f"\\ln(\\text{{turbidez}}) = {ecuacion_latex} + {intercept_prom:.4f}"

from IPython.display import display, Math
display(Math(ecuacion_latex))
```

**Escala real**

```{python}
#| echo: false
#| warning: false
#| message: false

# Transformo a escala real

y_train_pred_real = np.exp(y_train_pred)
y_test_pred_real = np.exp(y_test_pred)

y_train_real = np.exp(y_train)
y_test_real = np.exp(y_test)

rmse_test_real = np.sqrt(mean_squared_error(y_test_real, y_test_pred_real))
r2_test_real = r2_score(y_test_real, y_test_pred_real)

#Grafico

plt.figure(figsize=(4.5, 4.5))

# Entrenamiento (en escala real)
plt.scatter(y_train_real, y_train_pred_real, color="#9D50A6", alpha=0.5, label="Entrenamiento")

# Test (en escala real)
plt.scatter(y_test_real, y_test_pred_real, color="red", alpha=0.7, label="Test")

# Línea ideal
min_val = min(y_train_real.min(), y_test_real.min())
max_val = max(y_train_real.max(), y_test_real.max())
plt.plot([min_val, max_val], [min_val, max_val], '--', color="#17A77E", lw=2, label="Línea ideal")

# Línea de tendencia sobre test
coef_linea = np.polyfit(y_test, y_test_pred, 1)  # [pendiente, intercepto]
x_tendencia = np.linspace(min_val, max_val, 100)
y_tendencia = coef_linea[0] * x_tendencia + coef_linea[1]
plt.plot(x_tendencia, y_tendencia, '-', color='black', lw=2, label="Línea de tendencia")

plt.xlabel("Turbidez real (NTU)")
plt.ylabel("Turbidez estimada (NTU)")
plt.title(
    f"Regresión lineal en escala real\n"
    f"R² testeo: {r2_test_real:.4f}, RMSE: {rmse_test_real:.4f} NTU"
)
plt.legend()
plt.grid(True, linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()
```

### Modelo lineal 4

Utiliza el logaritmo de la turbidez y en las bandas: B04, B05 y B06

**Escala Logarítmica**
```{python}
#| echo: false
#| warning: false
#| message: false
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from bootstrap import run_bootstrap_linear_regression_analysis

# Leer datos
df = pd.read_csv(r"D:\GIT\Turbidez\Output\csv\ln_turb-ln_banda.csv")

# Variables predictoras
variables = ['ln_B05','ln_B04','ln_B06']
X_completo = df[variables].values
y_completo = df['ln_turbidez'].values

# Separar entrenamiento y testeo
X_train, X_test, y_train, y_test = train_test_split(
    X_completo, y_completo, test_size=0.25, random_state=42
)

# Ejecutar bootstrapping
n_iteraciones_config = 1000
coef_prom, intercept_prom, r2_train_boot, rmse_train_boot = \
    run_bootstrap_linear_regression_analysis(X_train, y_train, n_iteraciones=n_iteraciones_config)

# Modelo final con coeficientes promedio
modelo_final_promedio = LinearRegression()
modelo_final_promedio.coef_ = coef_prom
modelo_final_promedio.intercept_ = intercept_prom

# Predicción sobre entrenamiento y test
y_train_pred = modelo_final_promedio.predict(X_train)
y_test_pred = modelo_final_promedio.predict(X_test)

# Métricas en testeo
r2_test = r2_score(y_test, y_test_pred)
rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))

# ------------------ Gráfico con estilo personalizado ------------------ #
plt.figure(figsize=(4.5, 4.5))

# Entrenamiento
plt.scatter(y_train, y_train_pred, color="#9D50A6", alpha=0.5, label="Datos de entrenamiento", marker='o')

# Test
plt.scatter(y_test, y_test_pred, color="red", alpha=0.7, label="Datos de testeo", marker='^')

# Línea ideal
min_val = min(np.min(y_train), np.min(y_test))
max_val = max(np.max(y_train), np.max(y_test))
plt.plot([min_val, max_val], [min_val, max_val], '--', color="#17A77E", lw=2, label="Línea ideal")

# Línea de tendencia sobre test
coef_linea = np.polyfit(y_test, y_test_pred, 1)  # [pendiente, intercepto]
x_tendencia = np.linspace(min_val, max_val, 100)
y_tendencia = coef_linea[0] * x_tendencia + coef_linea[1]
plt.plot(x_tendencia, y_tendencia, '-', color='black', lw=2, label="Línea de tendencia")

plt.xlabel("ln(Turbidez) medida (NTU)")
plt.ylabel("ln(Turbidez) estimada (NTU)")
plt.title(
    f"Regresión lineal con Bootstrapping ({n_iteraciones_config} modelos)\n"
    f"R² entrenamiento: {r2_train_boot:.4f}, RMSE: {rmse_train_boot:.4f} | "
    f"R² testeo: {r2_test:.4f}, RMSE: {rmse_test:.4f}"
)
plt.legend()
plt.grid(True, linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

# ------------------ Coeficientes ------------------ #

# Diccionario para renombrar las variables en notación matemática
nombre_vars_latex = {
    'ln_B05': 'ln(B05)',
    'ln_B04': 'ln(B04)',
    'ln_B06': 'ln(B06)'
}

# Construir términos con nombres renombrados
terminos_latex = [
    f"{coef:.4f} \\cdot {nombre_vars_latex[var]}"
    for var, coef in zip(variables, coef_prom)
]

# Construir la ecuación completa en LaTeX
ecuacion_latex = " + ".join(terminos_latex)
ecuacion_latex = f"\\ln(\\text{{turbidez}}) = {ecuacion_latex} + {intercept_prom:.4f}"

from IPython.display import display, Math
display(Math(ecuacion_latex))

# Ecuacion real guardada como string
# Exponenciar el intercepto
intercepto_escala_real = np.exp(intercept_prom)

# Armar términos con potencias
terminos_real = [
    f"{var[3:]}^{{{coef:.4f}}}"  # Elimina "ln_" del nombre de la banda
    for var, coef in zip(variables, coef_prom)
]

# Construir la ecuación en LaTeX como string reutilizable
ecuacion_real_latex = f"\\text{{turbidez}} = {intercepto_escala_real:.4f} \\cdot " + " \\cdot ".join(terminos_real)

# (Opcional) Mostrar
from IPython.display import Math, display
display(Math(ecuacion_real_latex))

# Guardar en otra variable si querés reutilizar luego como string sin formato LaTeX
ecuacion_turb = f"turbidez = {intercepto_escala_real:.4f} * " + " * ".join([
    f"{var[3:]}^{coef:.4f}" for var, coef in zip(variables, coef_prom)
])

```

**Escala Real**

```{python}
#| echo: false
#| warning: false
#| message: false

# Transformo a escala real

y_train_pred_real = np.exp(y_train_pred)
y_test_pred_real = np.exp(y_test_pred)

y_train_real = np.exp(y_train)
y_test_real = np.exp(y_test)

rmse_test_real = np.sqrt(mean_squared_error(y_test_real, y_test_pred_real))
r2_test_real = r2_score(y_test_real, y_test_pred_real)

#Grafico

plt.figure(figsize=(4.5, 4.5))

# Entrenamiento (en escala real)
plt.scatter(y_train_real, y_train_pred_real, color="#9D50A6", alpha=0.5, label="Entrenamiento")

# Test (en escala real)
plt.scatter(y_test_real, y_test_pred_real, color="red", alpha=0.7, label="Test")

# Línea ideal
min_val = min(y_train_real.min(), y_test_real.min())
max_val = max(y_train_real.max(), y_test_real.max())
plt.plot([min_val, max_val], [min_val, max_val], '--', color="#17A77E", lw=2, label="Línea ideal")

# Línea de tendencia sobre test
coef_linea = np.polyfit(y_test, y_test_pred, 1)  # [pendiente, intercepto]
x_tendencia = np.linspace(min_val, max_val, 100)
y_tendencia = coef_linea[0] * x_tendencia + coef_linea[1]
plt.plot(x_tendencia, y_tendencia, '-', color='black', lw=2, label="Línea de tendencia")

plt.xlabel("Turbidez real (NTU)")
plt.ylabel("Turbidez estimada (NTU)")
plt.title(
    f"Regresión lineal en escala real\n"
    f"R² testeo: {r2_test_real:.4f}, RMSE: {rmse_test_real:.4f} NTU"
)
plt.legend()
plt.grid(True, linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()
```

### Modelo lineal elegido

Luego del anális de las distantas pruebas de correlación con diferenctes bandas y combinaciones de ellas, haciendo un balance entre complejidad del modelo y métricas de desempeño, se opta por elegir el **Modelo lineal 1**, que utiliza la B05 como variable predictora de la turbidez y será comparado con algún método de aprendizaje automático próximamente.

Si bien el Modelo lineal 2 (que usa 6 bandas) tiene mejores métricas de desempeño, la incorporacion de tantas bandas lo hace muy complejo. Los modelos que usan transformación logaritmica tienen un peor desempeño, respecto a los modelos sin transformaciones.

A continuación se observa la ecuación del modelo, métricas de desempeño y mapas de turbidez:

**Modelo con B05 y sus métricas**

```{python,}
#| echo: false
#| warning: false
#| message: false
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from bootstrap import run_bootstrap_linear_regression_analysis

# Leer datos
df = pd.read_csv(r"D:\GIT\Turbidez\Output\csv\Turbidez-Bandas.csv")

# Variables predictoras
variables = ['B05']
X_completo = df[variables].values
y_completo = df['turbidez'].values

# Separar entrenamiento y testeo
X_train, X_test, y_train, y_test = train_test_split(
    X_completo, y_completo, test_size=0.25, random_state=42
)

# Ejecutar bootstrapping
n_iteraciones_config = 1000
coef_prom, intercept_prom, r2_train_boot, rmse_train_boot = \
    run_bootstrap_linear_regression_analysis(X_train, y_train, n_iteraciones=n_iteraciones_config)

# ------------------ Guardar coeficientes del modelo B05 ------------------ #
coef_B05 = coef_prom[0]         # coeficiente de la banda B05
intercept_B05 = intercept_prom  # intercepto

# Ecuación en LaTeX (solo para mostrar)
ecuacion_latexB05 = f"turbidez = {coef_B05:.4f} \\cdot B05 + {intercept_B05:.4f}"

from IPython.display import display, Math
display(Math(ecuacion_latexB05))


# Modelo final con coeficientes promedio
modelo_final_promedio = LinearRegression()
modelo_final_promedio.coef_ = coef_prom
modelo_final_promedio.intercept_ = intercept_prom

# Predicción sobre entrenamiento y test
y_train_pred = modelo_final_promedio.predict(X_train)
y_test_pred = modelo_final_promedio.predict(X_test)

# Métricas en testeo
r2_test = r2_score(y_test, y_test_pred)
rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))

# ------------------ Gráfico con estilo personalizado ------------------ #
plt.figure(figsize=(4.5, 4.5))

# Entrenamiento
plt.scatter(y_train, y_train_pred, color="#9D50A6", alpha=0.5, label="Datos de entrenamiento", marker='o')

# Test
plt.scatter(y_test, y_test_pred, color="red", alpha=0.7, label="Datos de testeo", marker='^')

# Línea ideal
min_val = min(np.min(y_train), np.min(y_test))
max_val = max(np.max(y_train), np.max(y_test))
plt.plot([min_val, max_val], [min_val, max_val], '--', color="#17A77E", lw=2, label="Línea ideal")

# Línea de tendencia sobre test
coef_linea = np.polyfit(y_test, y_test_pred, 1)  # [pendiente, intercepto]
x_tendencia = np.linspace(min_val, max_val, 100)
y_tendencia = coef_linea[0] * x_tendencia + coef_linea[1]
plt.plot(x_tendencia, y_tendencia, '-', color='black', lw=2, label="Línea de tendencia")

plt.xlabel("Turbidez real (NTU)")
plt.ylabel("Turbidez estimada (NTU)")
plt.title(
    f"Regresión lineal con Bootstrapping ({n_iteraciones_config} modelos)\n"
    f"R² entrenamiento: {r2_train_boot:.4f}, RMSE: {rmse_train_boot:.4f} | "
    f"R² testeo: {r2_test:.4f}, RMSE: {rmse_test:.4f}"
)
plt.legend()
plt.grid(True, linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

# ------------------ Coeficientes ------------------ #

# Diccionario para renombrar las variables en notación matemática
nombre_vars_latex = {
    'B05': 'B05'
}

# Construir términos con nombres renombrados
terminos_latex = [
    f"{coef:.4f} \\cdot {nombre_vars_latex[var]}"
    for var, coef in zip(variables, coef_prom)
]

# Construir la ecuación completa en LaTeX
ecuacion_latexB05 = " + ".join(terminos_latex)
ecuacion_latexB05 = f"turbidez = {ecuacion_latexB05} + {intercept_prom:.4f}"

display(Math(ecuacion_latexB05))

```

**Mapas de turbidez modelo lineal elegido**

```{python}
#| echo: false
#| warning: false
#| message: false
import rasterio as rs
import numpy as np
import matplotlib.pyplot as plt
import glob
import os
from skimage.filters import threshold_otsu

# Carpeta con archivos .tif
carpeta = r"D:\GIT\Turbidez\Input\recorte_acolite"
salida = r"D:\GIT\Turbidez\Output\tiff mod_lineal"

# Crear carpeta si no existe
os.makedirs(salida, exist_ok=True)

# Buscar todos los .tif
tifs = sorted(glob.glob(os.path.join(carpeta, "*.tif")))
print(f"Se procesaron {len(tifs)} archivos .tif")

for ruta in tifs:
    nombre = os.path.basename(ruta)
    fecha = os.path.splitext(nombre)[0]   # nombre sin extensión
    
    with rs.open(ruta) as raster:
        # Bandas necesarias
        B03 = raster.read(3).astype(np.float32)  # Verde
        B05 = raster.read(5).astype(np.float32)  # RedEdge
        B11 = raster.read(10).astype(np.float32) # SWIR

        # Guardar perfil original
        perfil = raster.profile

    # --- NDWI ---
    ndwi = (B03 - B11) / (B03 + B11 + 1e-6)

    # Calcular el promedio (ignorando NaN si hay)
    umbral = np.nanmean(ndwi)

    # Binarizar con la media
    mascara = (ndwi > umbral).astype(np.uint8)

    # --- Turbidez ---
    turbidez = coef_B05 * B05 + intercept_B05
    turbidez = np.clip(turbidez, 0, None)  # negativos a 0
    turbidez_agua = np.where(mascara == 1, turbidez, np.nan)

    # --- Exportar a GeoTIFF ---
    salida_tif = os.path.join(salida, f"turbidez_{fecha}.tif")

    perfil.update(
        dtype=rs.float32,
        count=1,  # solo una banda (turbidez)
        compress='lzw',
        nodata=np.nan
    )

    with rs.open(salida_tif, 'w', **perfil) as dst:
        dst.write(turbidez_agua.astype(np.float32), 1)


    # --- Calcular percentiles para ajustar visualización ---
    p5, p95 = np.nanpercentile(turbidez_agua, [5, 95])

    # --- Gráfico conjunto ---
    fig, axes = plt.subplots(1, 2, figsize=(8, 4.5))

    # Panel 1: máscara de agua
    im1 = axes[0].imshow(mascara, cmap="Greys")
    axes[0].set_title(f"Máscara de agua - {fecha}")
    axes[0].axis("off")
    cbar1 = fig.colorbar(im1, ax=axes[0], fraction=0.046, pad=0.04)
    cbar1.set_label("Agua (1) / No Agua (0)")

    # Panel 2: turbidez
    im2 = axes[1].imshow(turbidez_agua, cmap="rainbow", vmin=p5, vmax=p95)
    axes[1].set_title(f"Turbidez - {fecha}")
    axes[1].axis("off")
    cbar2 = fig.colorbar(im2, ax=axes[1], fraction=0.046, pad=0.04)
    cbar2.set_label("Turbidez (NTU)")

    plt.tight_layout()
    plt.show()

```

# Entrenamiento de modelo de aprendizaje automático

## Modelo XGBoost 

Se seleccionó XGBoost, como modelo de aprendizaje automático para la estimación de la turbidez. Se importará el modelo desde sklearn, para su entrenamiento y conocer sus hiperparámetros. En esta etapa se utilizará XGBoost sin optimizacion de hiperparámetros, en una etapa posterior se elegirá un método de optimizacion y se evalurá nuevamente las métricas de desempeño.

**Entrenamiento** 

```{python}
#| code-fold: true
#| warning: false
#| message: false
#| eval: false
from xgboost import XGBRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error
import pandas as pd
import numpy as np
import os

# Leer datos

df = pd.read_csv(r"D:\GIT\Turbidez\Output\csv\Turbidez-Bandas.csv")

X = df[["B01","B02","B03","B04","B05","B06","B07","B08","B11","B12","B8A"]]
y = df["turbidez"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# Modelo XGBoost SIN optimizar

model = XGBRegressor(
n_estimators=500,
learning_rate=0.05,
max_depth=6,
subsample=0.8,
colsample_bytree=0.8,
random_state=42)

model.fit(X_train, y_train)

# Guardar modelo

ruta = r"D:\GIT\Turbidez\Output\Modelos"
os.makedirs(ruta, exist_ok=True)
model.save_model(os.path.join(ruta, "modelo_xgboost_sin_optimizar.json"))
```

```{python}
#| echo: false
#| warning: false
#| message: false
#| label: cargar_modelo_sin_optimizar

# Carga el modelo XGBoost sin optimizar

from xgboost import XGBRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error
import pandas as pd
import numpy as np

df = pd.read_csv(r"D:\GIT\Turbidez\Output\csv\Turbidez-Bandas.csv")

X = df[["B01","B02","B03","B04","B05","B06","B07","B08","B11","B12","B8A"]]
y = df["turbidez"]

X_train, X_test, y_train, y_test = train_test_split(
X, y, test_size=0.25, random_state=42
)

# Cargar modelo guardado

model = XGBRegressor()
model.load_model(r"D:\GIT\Turbidez\Output\Modelos\modelo_xgboost_sin_optimizar.json")

# Predicciones

y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)

# Métricas

r2_train = r2_score(y_train, y_train_pred)
rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))

r2_test = r2_score(y_test, y_test_pred)
rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))

```

**Métricas de desempeño**

```{python}
#| echo: false
#| warning: false
#| message: false

import matplotlib.pyplot as plt
import numpy as np

plt.figure(figsize=(4.5, 4.5))

plt.scatter(y_train, y_train_pred, color="#9D50A6", alpha=0.5,
label="Datos de entrenamiento", marker='o')

plt.scatter(y_test, y_test_pred, color="red", alpha=0.7,
label="Datos de testeo", marker='^')

min_val = min(np.min(y_train), np.min(y_test))
max_val = max(np.max(y_train), np.max(y_test))

plt.plot([min_val, max_val], [min_val, max_val], '--',
color="#17A77E", lw=2, label="Línea ideal")

coef = np.polyfit(y_test, y_test_pred, 1)
x_t = np.linspace(min_val, max_val, 100)
y_t = coef[0] * x_t + coef[1]
plt.plot(x_t, y_t, '-', color='black', lw=2, label="Línea de tendencia (test)")

plt.xlabel("Turbidez real (NTU)")
plt.ylabel("Turbidez estimada (NTU)")

plt.title(
f"Modelo XGBoost sin optimizar\n"
f"R² train: {r2_train:.3f}, RMSE:{rmse_train:.3f} | "
f"R² test: {r2_test:.3f}, RMSE:{rmse_test:.3f}"
)

plt.legend()
plt.grid(True, linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

```

```{python}
#| eval: false
#| echo: false
#| warning: false
#| message: false

import rasterio as rs
import numpy as np
import matplotlib.pyplot as plt
import glob
import os
from skimage.filters import threshold_otsu
import pandas as pd
from xgboost import XGBRegressor

# Cargar modelo XGBoost guardado
model = XGBRegressor()
model.load_model(r"D:\GIT\Turbidez\Output\Modelos\modelo_xgboost_sin_optimizar.json")

# --- Configuración ---
carpeta = r"D:\GIT\Turbidez\Input\recorte_acolite"
salida = r"D:\GIT\Turbidez\Output\tiff XGBoost"
os.makedirs(salida, exist_ok=True)

# Definir nombres de features (idénticos a entrenamiento)
feature_names = ["B01", "B02", "B03", "B04", "B05",
                 "B06", "B07", "B08", "B11", "B12", "B8A"]

# Buscar todos los .tif
tifs = sorted(glob.glob(os.path.join(carpeta, "*.tif")))
print(f"Se encontraron {len(tifs)} archivos .tif")

for ruta in tifs:
    nombre = os.path.basename(ruta)
    fecha = os.path.splitext(nombre)[0]


    with rs.open(ruta) as raster:
        # Leer bandas
        B01 = raster.read(1).astype(np.float32)
        B02 = raster.read(2).astype(np.float32)
        B03 = raster.read(3).astype(np.float32)
        B04 = raster.read(4).astype(np.float32)
        B05 = raster.read(5).astype(np.float32)
        B06 = raster.read(6).astype(np.float32)
        B07 = raster.read(7).astype(np.float32)
        B08 = raster.read(8).astype(np.float32)
        B8A = raster.read(9).astype(np.float32)
        B11 = raster.read(10).astype(np.float32)
        B12 = raster.read(11).astype(np.float32)

        perfil = raster.profile

    # --- NDWI y máscara de agua ---
    ndwi = (B03 - B11) / (B03 + B11 + 1e-6)
    umbral = np.nanmean(ndwi)
    mascara = (ndwi > umbral).astype(np.uint8)

    # --- Crear array de features ---
    filas, cols = B03.shape
    X_img = np.stack([B01, B02, B03, B04, B05,
                      B06, B07, B08, B11, B12, B8A], axis=-1)
    X_img_2d = X_img.reshape(-1, X_img.shape[-1])

    # Convertir a DataFrame con nombres de columnas iguales al entrenamiento
    X_img_2d_df = pd.DataFrame(X_img_2d, columns=feature_names)

    # --- Predecir turbidez ---
    turbidez_pred = model.predict(X_img_2d_df)
    turbidez_pred = turbidez_pred.reshape(filas, cols)

    # Aplicar máscara de agua
    turbidez_agua = np.where(mascara == 1, turbidez_pred, np.nan)

    # --- Exportar a GeoTIFF ---
    salida_tif = os.path.join(salida, f"turbidez_{fecha}.tif")
    perfil.update(dtype=rs.float32, count=1, compress='lzw', nodata=np.nan)
    with rs.open(salida_tif, 'w', **perfil) as dst:
        dst.write(turbidez_agua.astype(np.float32), 1)

    # --- Percentiles para visualización ---
    p5, p95 = np.nanpercentile(turbidez_agua, [5, 95])

    # --- Gráfico ---
    fig, axes = plt.subplots(1, 2, figsize=(8, 4.5))
    im1 = axes[0].imshow(mascara, cmap="Greys")
    axes[0].set_title(f"Máscara de agua - {fecha}")
    axes[0].axis("off")
    cbar1 = fig.colorbar(im1, ax=axes[0], fraction=0.046, pad=0.04)
    cbar1.set_label("Agua (1) / No Agua (0)")

    im2 = axes[1].imshow(turbidez_agua, cmap="rainbow", vmin=p5, vmax=p95)
    axes[1].set_title(f"Turbidez (XGBoost) - {fecha}")
    axes[1].axis("off")
    cbar2 = fig.colorbar(im2, ax=axes[1], fraction=0.046, pad=0.04)
    cbar2.set_label("Turbidez (NTU)")

    plt.tight_layout()
    plt.show()

```


## Hiperparámetros utilizados

• n_estimators=500 → número de arboles que se entrenan secuencialmente

• learning_rate=0.05 → indica que tanta aprende cada arbol

Es el “paso” con que el modelo corrige los errores.
-Valores bajos (0.01–0.1) → aprendizaje más lento pero más estable.
-Valores altos (>0.3) → puede sobreajustar rápido.

• max_depth=6 → Profundidad máxima de cada árbol.

Controla la complejidad de las interacciones entre las variables (bandas).
Más profundo → aprende patrones complejos, pero también ruido.

• subsample=0.8 → Fracción de los datos usados para entrenar cada árbol.

Introduce aleatoriedad. Ayuda a reducir el sobreajuste, 0.8 significa que cada árbol usa el 80% de las filas del dataset.

• colsample_bytree=0.8 → Fracción de las variables (columnas) usadas para cada árbol. Igual que subsample, pero sobre las bandas.

Esto fuerza a los árboles a usar combinaciones distintas de bandas, lo que mejora la generalización.

0.8 = buena práctica para evitar que el modelo dependa demasiado de una sola banda.

Los hiperparámetros controlan el comportamiento del modelo, pero no se aprenden automáticamente del conjunto de datos.
Por eso, necesitamos un método que busque las mejores combinaciones posibles para obtener el máximo rendimiento sin sobreajustar.

## Optimización de hiperparámetros

Existen muchos métodos de optimizacion de hiperparámetros

1. **Grid Search (búsqueda en rejilla)**

Prueba todas las combinaciones posibles de los valores que uno defina.

Por ejemplo, si se prueba 3 valores de max_depth y 3 de learning_rate, va a entrenar 3 × 3 = 9 modelos.

Usa validación cruzada (cross-validation) para evaluar cada combinación.

*Ventajas*

  • Simple, exhaustivo y fácil de entender.

  • Ideal cuando el número de combinaciones es pequeño.

*Desventajas*

  • Muy lento si probás muchos parámetros o valores.

  • No aprende de los resultados anteriores (prueba todo a ciegas).

2. **Random Search (búsqueda aleatoria)**

En lugar de probar todas las combinaciones, elige al azar un número fijo de combinaciones.

Ejemplo: podrías probar 30 combinaciones elegidas aleatoriamente de un rango mucho más grande.

 *Ventajas*

  • Mucho más rápido que Grid Search.

  • Puede explorar un espacio grande de parámetros.

  • Buen punto medio entre precisión y costo computacional.

 *Desventajas*

  • Resultados algo aleatorios (aunque reproducibles con random_state).

  • No garantiza probar la “mejor” combinación posible.

3. **Búsqueda bayesiana inteligente (Optuna / Hyperopt / scikit-optimize)**

Usa inteligencia adaptativa: aprende de las pruebas anteriores y dirige las siguientes pruebas hacia las regiones más prometedoras.

Es una búsqueda inteligente, no aleatoria ni exhaustiva.

 *Ventajas*

 • Mucho más eficiente que Grid o Random Search.

 • Encuentra buenos resultados con menos intentos.

 • Ideal para modelos complejos como XGBoost.

 *Desventajas*

 • Requiere instalar librerías externas (como optuna).

 • Más técnica, aunque muy potente.

En este trabajo se optó por utilar Optuna para la optimizacion de los hiperparámetros

## Modelo XGBoost optimizado con Optuna (Búsqueda bayesiana inteligente)

```{python}
#| echo: false
#| warning: false
#| message: false
#| eval: false
#| label: entrenar_xgb_optuna

# Entrenamiento del modelo

from xgboost import XGBRegressor
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import r2_score, mean_squared_error
import pandas as pd
import numpy as np
import optuna
import os

# Leer datos
df = pd.read_csv(r"D:\GIT\Turbidez\Output\csv\Turbidez-Bandas.csv")

X = df[["B01","B02","B03","B04","B05","B06","B07","B08","B11","B12","B8A"]]
y = df["turbidez"]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.25, random_state=42
)

optuna.logging.set_verbosity(optuna.logging.WARNING)

def objective(trial):

    params = {
        'max_depth': trial.suggest_int('max_depth', 3, 10),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
        'subsample': trial.suggest_float('subsample', 0.6, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
        'n_estimators': trial.suggest_int('n_estimators', 200, 800),
        'random_state': 42
    }

    model = XGBRegressor(**params)
    score = cross_val_score(model, X_train, y_train, scoring='r2', cv=5).mean()
    return score

study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=50)

# Tabla con mejores hiperparámetros
best_params_df = pd.DataFrame([study.best_params])
best_params_df["R2_promedio_CV"] = study.best_value
display(best_params_df)

# Entrenar modelo final
model = XGBRegressor(**study.best_params)
model.fit(X_train, y_train)

# Guardar modelo y parámetros
ruta = r"D:\GIT\Turbidez\Output\Modelos"
os.makedirs(ruta, exist_ok=True)

model.save_model(os.path.join(ruta, "modelo_xgboost_optuna.json"))
best_params_df.to_csv(os.path.join(ruta, "xgb_optuna_params.csv"), index=False)

```

Tabla con hiperparámetros optimizados

```{python}
#| echo: false
#| warning: false
#| message: false
#| label: tabla_optuna

import pandas as pd

params_path = r"D:\GIT\Turbidez\Output\Modelos\xgb_optuna_params.csv"
best_params_df = pd.read_csv(params_path)

best_params_df

```

```{python}
#| echo: false
#| warning: false
#| message: false
#| label: cargar_modelo_xgb_optuna

# Carga el modelo entrenado

from xgboost import XGBRegressor
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error

# Cargar datos

df = pd.read_csv(r"D:\GIT\Turbidez\Output\csv\Turbidez-Bandas.csv")

X = df[["B01","B02","B03","B04","B05","B06","B07","B08","B11","B12","B8A"]]
y = df["turbidez"]

X_train, X_test, y_train, y_test = train_test_split(
X, y, test_size=0.25, random_state=42
)

model = XGBRegressor()
model.load_model(r"D:\GIT\Turbidez\Output\Modelos\modelo_xgboost_optuna.json")

# Predicciones

y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)

# Métricas

r2_train = r2_score(y_train, y_train_pred)
rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))

r2_test = r2_score(y_test, y_test_pred)
rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))

```

Métricas de desempeño

```{python}
#| echo: false
#| warning: false
#| message: false
#| label: xgb_optuna_plot


import matplotlib.pyplot as plt
import numpy as np

plt.figure(figsize=(4.5, 4.5))

plt.scatter(y_train, y_train_pred, color="#9D50A6", alpha=0.5,
label="Datos de entrenamiento", marker='o')

plt.scatter(y_test, y_test_pred, color="red", alpha=0.7,
label="Datos de testeo", marker='^')

min_val = min(np.min(y_train), np.min(y_test))
max_val = max(np.max(y_train), np.max(y_test))

plt.plot([min_val, max_val], [min_val, max_val], '--',
color="#17A77E", lw=2, label="Línea ideal")

coef = np.polyfit(y_test, y_test_pred, 1)
x_t = np.linspace(min_val, max_val, 100)
y_t = coef[0] * x_t + coef[1]
plt.plot(x_t, y_t, '-', color='black', lw=2, label="Línea de tendencia (test)")

plt.xlabel("Turbidez real (NTU)")
plt.ylabel("Turbidez estimada (NTU)")

plt.title(
f"Modelo XGBoost optimizado (Optuna)\n"
f"R² train: {r2_train:.3f}, RMSE:{rmse_train:.3f} | "
f"R² test: {r2_test:.3f}, RMSE:{rmse_test:.3f}"
)

plt.legend()
plt.grid(True, linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

```

**Mapas XGBoost Optuna**

```{python}
#| echo: false
#| warning: false
#| message: false

import rasterio as rs
import numpy as np
import matplotlib.pyplot as plt
import glob
import os
from skimage.filters import threshold_otsu
import pandas as pd
from xgboost import XGBRegressor

# Cargar modelo XGBoost guardado
model = XGBRegressor()
model.load_model(r"D:\GIT\Turbidez\Output\Modelos\modelo_xgboost_optuna.json")

# --- Configuración ---
carpeta = r"D:\GIT\Turbidez\Input\recorte_acolite"
salida = r"D:\GIT\Turbidez\Output\tiff XGBoost Optuna"
os.makedirs(salida, exist_ok=True)

# Definir nombres de features (idénticos a entrenamiento)
feature_names = ["B01", "B02", "B03", "B04", "B05",
                 "B06", "B07", "B08", "B11", "B12", "B8A"]

# Buscar todos los .tif
tifs = sorted(glob.glob(os.path.join(carpeta, "*.tif")))
print(f"Se procesaron {len(tifs)} archivos .tif")

for ruta in tifs:
    nombre = os.path.basename(ruta)
    fecha = os.path.splitext(nombre)[0]
    
    with rs.open(ruta) as raster:
        # Leer bandas
        B01 = raster.read(1).astype(np.float32)
        B02 = raster.read(2).astype(np.float32)
        B03 = raster.read(3).astype(np.float32)
        B04 = raster.read(4).astype(np.float32)
        B05 = raster.read(5).astype(np.float32)
        B06 = raster.read(6).astype(np.float32)
        B07 = raster.read(7).astype(np.float32)
        B08 = raster.read(8).astype(np.float32)
        B8A = raster.read(9).astype(np.float32)
        B11 = raster.read(10).astype(np.float32)
        B12 = raster.read(11).astype(np.float32)

        perfil = raster.profile

    # --- NDWI y máscara de agua ---
    ndwi = (B03 - B11) / (B03 + B11 + 1e-6)
    umbral = np.nanmean(ndwi)
    mascara = (ndwi > umbral).astype(np.uint8)

    # --- Crear array de features ---
    filas, cols = B03.shape
    X_img = np.stack([B01, B02, B03, B04, B05,
                      B06, B07, B08, B11, B12, B8A], axis=-1)
    X_img_2d = X_img.reshape(-1, X_img.shape[-1])

    # Convertir a DataFrame con nombres de columnas iguales al entrenamiento
    X_img_2d_df = pd.DataFrame(X_img_2d, columns=feature_names)

    # --- Predecir turbidez ---
    turbidez_pred = model.predict(X_img_2d_df)
    turbidez_pred = turbidez_pred.reshape(filas, cols)

    # Aplicar máscara de agua
    turbidez_agua = np.where(mascara == 1, turbidez_pred, np.nan)

    # --- Exportar a GeoTIFF ---
    salida_tif = os.path.join(salida, f"turbidez_{fecha}.tif")
    perfil.update(dtype=rs.float32, count=1, compress='lzw', nodata=np.nan)
    with rs.open(salida_tif, 'w', **perfil) as dst:
        dst.write(turbidez_agua.astype(np.float32), 1)

    # --- Percentiles para visualización ---
    p5, p95 = np.nanpercentile(turbidez_agua, [5, 95])

    # --- Gráfico ---
    fig, axes = plt.subplots(1, 2, figsize=(8, 4.5))
    im1 = axes[0].imshow(mascara, cmap="Greys")
    axes[0].set_title(f"Máscara de agua - {fecha}")
    axes[0].axis("off")
    cbar1 = fig.colorbar(im1, ax=axes[0], fraction=0.046, pad=0.04)
    cbar1.set_label("Agua (1) / No Agua (0)")

    im2 = axes[1].imshow(turbidez_agua, cmap="rainbow", vmin=p5, vmax=p95)
    axes[1].set_title(f"Turbidez (XGBoost) - {fecha}")
    axes[1].axis("off")
    cbar2 = fig.colorbar(im2, ax=axes[1], fraction=0.046, pad=0.04)
    cbar2.set_label("Turbidez (NTU)")

    plt.tight_layout()
    plt.show()
```

## Relevancia de Bandas espectrales en el modelo XGBOOST

La importancia de las bandas indica cuánto contribuye cada banda espectral (o combinación de ellas) al rendimiento del modelo.
Esto permite saber cuáles longitudes de onda son más informativas para estimar la variable objetivo.

Interpretación:

Cuanto mayor sea la importancia, más influye esa banda en las divisiones del modelo.

No necesariamente significa correlación directa (puede ser no lineal).

Si una banda tiene muy baja importancia (≈0), puede eliminarse en futuras versiones del modelo para simplificarlo.

Cuando entrenás un modelo como XGBoost, el algoritmo aprende qué variables (bandas espectrales) son más útiles para predecir la turbidez.

• La importancia de las bandas indica cuánto aportó cada banda al modelo para mejorar sus predicciones.

En XGBoost, el atributo model.feature_importances_ mide la importancia promedio de cada variable dentro de todos los árboles del modelo.

XGBoost da la importancia relativa (no absoluta ni porcentual) basada en "weight".
Los valores se normalizan para que la suma de todos sea 1.0.

| Tipo de importancia (`importance_type`) | Qué mide exactamente                                           | Interpretación práctica                       |
| --------------------------------------- | -------------------------------------------------------------- | --------------------------------------------- |
| `"weight"` *(por defecto)*              | Número de veces que la variable fue usada para dividir nodos   | Bandas usadas más veces para tomar decisiones |
| `"gain"`                                | Promedio de la mejora en la predicción al usar esa banda       | Cuánto “valor” aporta cada vez que se usa     |
| `"cover"`                               | Promedio de la cantidad de datos afectados por esas divisiones | Qué tan general es su influencia              |
| `"total_gain"` o `"total_cover"`        | Suma total de las métricas anteriores                          | Importancia acumulada 

**Importancia de cada banda dentro del modelo XGBoost**

```{python}

#| echo: false
#| warning: false
#| message: false

import pandas as pd
import matplotlib.pyplot as plt

# Obtener importancias del modelo entrenado
importancias = model.feature_importances_

# Crear DataFrame ordenado
importancia_df = pd.DataFrame({
    'Banda': X.columns,
    'Importancia': importancias
}).sort_values(by='Importancia', ascending=False)

importancia_df = importancia_df.reset_index(drop=True)

display(importancia_df)

# Gráfico de barras ordenado
plt.figure(figsize=(6, 4))
plt.bar(importancia_df['Banda'], importancia_df['Importancia'], color='#2E86AB')
plt.title('Importancia de las bandas en el modelo XGBoost')
plt.xlabel('Banda')
plt.ylabel('Importancia relativa')
plt.xticks(rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

```

El análisis de importancia de características del modelo XGBoost optimizado muestra una dominancia muy marcada de la banda B05, que concentra cerca del 60% de la importancia total. Esto indica que, para el modelo, la información contenida en B05 es el factor más determinante para estimar la turbidez.

En segundo lugar aparece la banda B06, con una importancia mucho menor pero aún relevante aproximadamente 15%.
Después de estas dos, el resto de las bandas (B01, B02, B08, B8A, B07) aportan importancias moderadas y similares entre sí, lo que indica que contribuyen al modelo pero de forma secundaria.

Por otro lado, algunas bandas como B03, B04, B11 y B12 tienen una importancia prácticamente nula, lo que sugiere que su aporte al modelo es mínimo. Esto podría deberse a que estas bandas no contienen información útil para diferenciar niveles de turbidez o dicha información está fuertemente correlacionada con otras bandas más relevantes.

El modelo depende fuertemente de un conjunto reducido de bandas, especialmente B05.

La alta concentración de importancia en pocas bandas sugiere que la señal espectral relacionada con la turbidez está localizada en rangos específicos del espectro.

Las bandas con importancia mínima podrían ser candidatas para eliminación en futuros experimentos, lo que implicaría: 
-Modelos más ligeros
-Menor costo computacional
-Potencialmente mejorar generalización al reducir ruido

# Conclusión

El análisis realizado demuestra que la estimación de turbidez a partir de bandas espectrales Sentinel-2 presenta un buen potencial predictivo, pero la capacidad del modelo depende fuertemente del tipo de algoritmo utilizado.

El modelo lineal seleccionado inicialmente permitió explorar la relación entre cada banda y la turbidez, mostrando que ciertas longitudes de onda —especialmente B05, B06 y B08— concentran la mayor parte de la información relevante. Sin embargo, el comportamiento real del fenómeno es notablemente no lineal, con interacciones entre bandas que un modelo lineal no logra capturar completamente. Esto se reflejó en un desempeño aceptable pero limitado, donde el modelo tendió a subestimar valores altos de turbidez y a producir una mayor dispersión en las predicciones.

En contraste, el XGBoost optimizado mediante Optuna mostró un salto significativo en capacidad predictiva. El proceso de optimización permitió ajustar hiperparámetros clave, logrando un modelo más robusto, con mejor generalización y con un ajuste más preciso tanto para los datos de entrenamiento como para los de testeo. Los indicadores R² y RMSE mejoraron sustancialmente respecto del modelo lineal, y esto se evidencia visualmente en el gráfico estimado vs real, donde la dispersión es mucho menor y la tendencia se ajusta mejor a la diagonal ideal.

Además, el análisis de importancia de características del XGBoost confirma y profundiza lo detectado con el modelo lineal: las bandas del espectro rojo y NIR (especialmente B05) son determinantes para explicar la turbidez. Sin embargo, el modelo no lineal también asigna pequeñas contribuciones a otras bandas que, aunque individualmente débiles, aportan información útil cuando interactúan entre sí, lo que el modelo lineal no puede aprovechar.

El modelo lineal es simple, interpretable y útil para comprender relaciones básicas entre bandas y turbidez, pero su precisión es limitada.

El XGBoost optimizado ofrece una mejora clara en desempeño, captura relaciones no lineales y reduce significativamente el error, mostrando ser el enfoque más adecuado para el problema.

La combinación de optimización automática (Optuna) y un algoritmo avanzado de ensamble permite obtener un modelo robusto y reproducible, adaptable a distintos conjuntos de datos y con potencial para implementarse operativamente.

En conclusión, el modelo XGBoost optimizado se posiciona como la mejor alternativa para estimar turbidez a partir de productos satelitales Sentinel-2, superando ampliamente al modelo lineal tanto en precisión como en capacidad de generalización.

